# Arquivo de configuração completo para o AllTrainer, com todos os trainers disponíveis.

env_settings:
  env_path: /Users/victorbarbosa/git/UNITY Web/UnityStarshipLaunchigLanding/MAC/App
  # Vamos criar 8 ambientes para mapear cada um para um trainer diferente
  num_envs: 8
  worker_trainer_types: [ppo, sac, td3, tdsac, tqc, poca, drqv2, dcac]
  num_areas: 1
  base_port: 5005
  timeout_wait: 60
  seed: -1

# Atribui um trainer específico para cada worker. A ordem é importante.
worker_trainer_types: [ppo, sac, td3, tdsac, tqc, poca, drqv2, dcac]

default_settings: null

behaviors:
  VerticalAgent:
    trainer_type: all
    hyperparameters:
      batch_size: 128
      buffer_size: 100000
      learning_rate: 0.001
      learning_rate_schedule: linear
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
      memory: null # Desabilitado no nível principal, mas pode ser habilitado em sub-trainers
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    max_steps: 100000000
    time_horizon: 256
    summary_freq: 1000
    keep_checkpoints: 50

    # --- Configurações Específicas para cada Sub-Trainer ---

    ppo:
      trainer_type: ppo
      hyperparameters:
        batch_size: 256
        buffer_size: 100000
        learning_rate: 0.001
        beta: 0.001
        epsilon: 0.2
        lambd: 0.95
        num_epoch: 3
        learning_rate_schedule: linear

    sac:
      trainer_type: sac
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        init_entcoef: 0.1

    td3:
      trainer_type: td3
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        policy_delay: 2

    tdsac:
      trainer_type: tdsac
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        init_entcoef: 0.1

    tqc:
      trainer_type: tqc
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        init_entcoef: 0.1
        # Parâmetros específicos do TQC
        n_quantiles: 25
        n_to_drop: 2

    poca:
      trainer_type: poca
      hyperparameters:
        batch_size: 256
        buffer_size: 100000
        learning_rate: 0.001
        beta: 0.001
        epsilon: 0.2
        lambd: 0.95
        num_epoch: 3
        learning_rate_schedule: linear

    drqv2:
      trainer_type: drqv2
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        init_entcoef: 0.1
        image_pad: 4

    dcac:
      trainer_type: dcac
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: false
        init_entcoef: 0.1
        destructive_threshold: 0.0

# As configurações abaixo são aplicadas a todos os ambientes
engine_settings:
  width: 250
  height: 250
  quality_level: 0
  time_scale: 100.0
  target_frame_rate: 60
  capture_frame_rate: 0
  no_graphics: true

checkpoint_settings:
  run_id: all_trainers_example
  force: true
  results_dir: results
