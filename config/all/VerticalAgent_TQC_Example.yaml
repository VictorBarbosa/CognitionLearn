env_settings:
  # Vamos criar 6 ambientes paralelos (workers) para demonstrar todos os trainers
  num_envs: 6

# Define qual trainer cada worker usar√°.
worker_trainer_types: [ppo, sac, tqc, td3, tdsac, poca]

default_settings: null
behaviors:
  VerticalAgent:
    trainer_type: all
    hyperparameters:
      batch_size: 128
      buffer_size: 100000
      learning_rate: 0.001
      learning_rate_schedule: linear
    checkpoint_interval: 5000
    network_settings:
      normalize: true
      hidden_units: 128
      num_layers: 2
      vis_encode_type: simple
      memory:
        memory_size: 128
        sequence_length: 64
      goal_conditioning_type: hyper
      deterministic: false
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
        network_settings:
          normalize: true
          hidden_units: 128
          num_layers: 2
          vis_encode_type: simple
          memory:
            memory_size: 128
            sequence_length: 64
          goal_conditioning_type: hyper
          deterministic: false
    keep_checkpoints: 50
    max_steps: 100000000
    time_horizon: 256
    summary_freq: 1000
    threaded: false

    # --- Sub-trainer configurations ---

    ppo:
      trainer_type: ppo
      hyperparameters:
        batch_size: 256
        buffer_size: 100000
        learning_rate: 0.001
        beta: 0.001
        epsilon: 0.2
        lambd: 0.95
        num_epoch: 3
        learning_rate_schedule: linear
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64

    sac:
      trainer_type: sac
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: true
        init_entcoef: 0.1
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64

    td3:
      trainer_type: td3
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: true
        policy_delay: 2
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64

    tdsac:
      trainer_type: tdsac
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: true
        init_entcoef: 0.1
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64

    tqc:
      trainer_type: tqc
      hyperparameters:
        learning_rate: 0.001
        learning_rate_schedule: linear
        batch_size: 256
        buffer_size: 100000
        buffer_init_steps: 10000
        tau: 0.005
        steps_per_update: 1.0
        save_replay_buffer: true
        init_entcoef: 0.1
        # TQC specific
        n_quantiles: 25
        n_to_drop: 2
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64

    poca:
      trainer_type: poca
      hyperparameters:
        batch_size: 256
        buffer_size: 100000
        learning_rate: 0.001
        beta: 0.001
        epsilon: 0.2
        lambd: 0.95
        num_epoch: 3
        learning_rate_schedule: linear
      network_settings:
        normalize: true
        hidden_units: 128
        num_layers: 2
        memory:
          memory_size: 128
          sequence_length: 64