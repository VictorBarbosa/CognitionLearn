behaviors:
  # Nome do comportamento deve corresponder ao nome no ambiente Unity
  YourBehaviorName:
    trainer_type: dreamerv3
    hyperparameters:
      # Hiperparâmetros do DreamerV3
      wm_hidden_size: 400
      wm_latent_state_size: 60
      wm_stochastic_state_size: 32
      wm_reward_buckets: 1
      
      actor_hidden_size: 400
      actor_discrete_temperature: 1.0
      
      critic_hidden_size: 400
      critic_ensemble_size: 4
      
      imagination_horizon: 15
      discount: 0.99
      lambda_return: 0.95
      world_model_loss_scale: 1.0
      reward_loss_scale: 1.0
      continue_loss_scale: 1.0
      kl_loss_scale: 1.0
      kl_free_nats: 3.0
      kl_free_avg: true
      actor_lr: 0.00008  # 8e-5
      critic_lr: 0.00008  # 8e-5
      world_model_lr: 0.0003  # 3e-4
      grad_clip: 100.0
      value_target_tau: 0.005
      
      # Hiperparâmetros padrão do ML-Agents
      batch_size: 50
      buffer_size: 1000000
      steps_per_update: 0.5
      buffer_init_steps: 0
      save_replay_buffer: false
      reward_signal_steps_per_update: 4

    network_settings:
      normalize: true
      hidden_units: 512
      num_layers: 3
      vis_encode_type: simple
      memory:
        sequence_length: 64
        memory_size: 256

    # Configurações de recompensa (opcional)
    reward_signals:
      extrinsic:
        strength: 1.0
        gamma: 0.99

    # Configurações do treinamento
    max_steps: 1000000
    time_horizon: 128
    summary_freq: 10000
    checkpoint_interval: 500000
    keep_checkpoints: 5
    threaded: false
    self_play: null
    behavioral_cloning: null